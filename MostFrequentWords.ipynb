{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script, I am checking which words are the most frequent ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import folder_meicogsci\n",
    "import collections\n",
    "import string\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(folder_meicogsci)\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "lem = nltk.stem.WordNetLemmatizer()\n",
    "stem = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_words_all_words_noprocess = collections.defaultdict(dict)\n",
    "for filename in all_files:\n",
    "    text_name = filename.split(\"_\")[-1].replace(\".txt\", \"\")\n",
    "    with open(os.path.join(folder_meicogsci, filename)) as f:\n",
    "        data_file_raw = f.readlines()\n",
    "    # the following part is here to eliminate the rows, that are just citations, since they are providing noise\n",
    "    data_file = data_file_raw[:-7][:]\n",
    "    for row in data_file_raw[-7:][:]:\n",
    "        row = row.strip()\n",
    "        if len(row) > 2:\n",
    "            if not row[0] == \"[\" and not row[2] == \"]\":\n",
    "                data_file.append(row)\n",
    "    # the following part is there to eliminate the rown, that are just thanks\n",
    "    data_file = [row for row in data_file if \"special thanks\" not in row.lower()]\n",
    "    data_file = [row for row in data_file if \"acknowledgements\" not in row.lower()]\n",
    "    data_file = [row for row in data_file if \"I would like to thank\" not in row.lower()]\n",
    "    data_file = [row for row in data_file if \"we thank\" not in row.lower()]\n",
    "    data_file = [row for row in data_file if \"special thanks\" not in row.lower()]\n",
    "    data_file = [row for row in data_file if \"sincere thanks\" not in row.lower()]\n",
    "    data_file = [row for row in data_file if \"many thanks\" not in row.lower()]\n",
    "    # the following part is there to eliminate the abstract and reference header\n",
    "    data_file = [row for row in data_file if \"abstract\" not in row.lower() or len(row) > 25]\n",
    "    data_file = [row for row in data_file if \"reference\" not in row.lower() or len(row) > 25]\n",
    "    data_file = [row for row in data_file if \"university press\" not in row.lower()]\n",
    "    # the the text is being put in the format, that makes it more informative to compare\n",
    "    data_file = \" \".join(data_file)\n",
    "    data_file = data_file.replace(\"... title:\", \"\")\n",
    "    data_file = data_file.lower()\n",
    "    data_file = data_file.replace(\"-\", \" \").replace(\"/\", \" \").replace(\"–\", \" \") \n",
    "    # here, the test is being split into words and compared based on that\n",
    "    data_words = nltk.tokenize.word_tokenize(data_file)\n",
    "    data_words = [w.strip() for w in data_words if w.strip()]\n",
    "    # this is prefiltering based on 1-grams\n",
    "    data_words_filtered_nltk = nltk.pos_tag(data_words)\n",
    "    data_words_filtered = [w for w, pos in data_words_filtered_nltk if pos[:2] in {\"JJ\", \"NP\"}]\n",
    "    data_words_filtered = [w for w in data_words_filtered if w not in stop_words and w not in string.punctuation and w not in \"`’”“\"]\n",
    "    data_words_filtered = [w for w in data_words_filtered if len(w) > 2]\n",
    "    data_words_filtered = [lem.lemmatize(w) for w in data_words_filtered]\n",
    "    text_words_all_words_noprocess[text_name][1] = data_words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_one_grams = []\n",
    "for text_id, info in text_words_all_words_noprocess.items():\n",
    "    all_one_grams += set(info[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_one_grams = collections.Counter(all_one_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('different', 407),\n",
       " ('cognitive', 339),\n",
       " ('first', 264),\n",
       " ('human', 245),\n",
       " ('possible', 212),\n",
       " ('many', 195),\n",
       " ('specific', 191),\n",
       " ('new', 188),\n",
       " ('main', 185),\n",
       " ('important', 179),\n",
       " ('second', 174),\n",
       " ('social', 163),\n",
       " ('various', 146),\n",
       " ('similar', 146),\n",
       " ('previous', 144),\n",
       " ('neural', 141),\n",
       " ('visual', 139),\n",
       " ('able', 138),\n",
       " ('current', 134),\n",
       " ('higher', 130)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_one_grams.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
