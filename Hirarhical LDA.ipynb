{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hirarhical LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here instead of LDA I used hierarchical dirichlet process. I was surprised by two things. First, how much quicker it was then LDA and second, how less interpretable results did I get. (But if I learned anything during this project it is, that my knowledge of cognitive sciences is limited)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "import pandas\n",
    "import collections\n",
    "from constants import all_topics_names, folder_meicogsci, folder_meicogsci_2019, folder_models, folder_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of abstracts\n",
    "all_files = os.listdir(folder_meicogsci)\n",
    "all_files_2019 = os.listdir(folder_meicogsci_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get helping functions\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "lem = nltk.stem.WordNetLemmatizer()\n",
    "stem = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the text\n",
    "text_words_all_words_noprocess = collections.defaultdict(dict)\n",
    "for folder, filenames in [[folder_meicogsci, all_files], [folder_meicogsci_2019, all_files_2019]]:\n",
    "    for filename in filenames:\n",
    "        text_name = filename.split(\"_\")[-1].replace(\".txt\", \"\")\n",
    "        with open(os.path.join(folder, filename)) as f:\n",
    "            data_file_raw = f.readlines()\n",
    "        # the following part is here to eliminate the rows, that are just citations, since they are providing noise\n",
    "        data_file = data_file_raw[:-7][:]\n",
    "        for row in data_file_raw[-7:][:]:\n",
    "            row = row.strip()\n",
    "            if len(row) > 2:\n",
    "                if not row[0] == \"[\" and not row[2] == \"]\":\n",
    "                    data_file.append(row)\n",
    "        # the following part is there to eliminate the rown, that are just thanks\n",
    "        data_file = [row for row in data_file if \"special thanks\" not in row.lower()]\n",
    "        data_file = [row for row in data_file if \"acknowledgements\" not in row.lower()]\n",
    "        data_file = [row for row in data_file if \"I would like to thank\" not in row.lower()]\n",
    "        data_file = [row for row in data_file if \"we thank\" not in row.lower()]\n",
    "        data_file = [row for row in data_file if \"special thanks\" not in row.lower()]\n",
    "        data_file = [row for row in data_file if \"sincere thanks\" not in row.lower()]\n",
    "        data_file = [row for row in data_file if \"many thanks\" not in row.lower()]\n",
    "        # the following part is there to eliminate the abstract and reference header\n",
    "        data_file = [row for row in data_file if \"abstract\" not in row.lower() or len(row) > 25]\n",
    "        data_file = [row for row in data_file if \"reference\" not in row.lower() or len(row) > 25]\n",
    "        data_file = [row for row in data_file if \"university press\" not in row.lower()]\n",
    "        # the the text is being put in the format, that makes it more informative to compare\n",
    "        data_file = \" \".join(data_file)\n",
    "        data_file = data_file.replace(\"... title:\", \"\")\n",
    "        data_file = data_file.lower()\n",
    "        data_file = data_file.replace(\"-\", \" \").replace(\"/\", \" \").replace(\"–\", \" \") \n",
    "        # here, the test is being split into words and compared based on that\n",
    "        data_words = nltk.tokenize.word_tokenize(data_file)\n",
    "        data_words = [w.strip() for w in data_words if w.strip()]\n",
    "        # this is prefiltering based on 1-grams\n",
    "        data_words_filtered_nltk = nltk.pos_tag(data_words)\n",
    "        data_words_filtered = [w for w, pos in data_words_filtered_nltk if pos[:2] in {\"JJ\", \"NN\"}]\n",
    "        data_words_filtered = [w for w in data_words_filtered if w not in stop_words and w not in string.punctuation and w not in \"`’”“\"]\n",
    "        data_words_filtered = [w for w in data_words_filtered if len(w) > 2]\n",
    "        data_words_filtered = [lem.lemmatize(w) for w in data_words_filtered]\n",
    "        text_words_all_words_noprocess[text_name][1] = data_words_filtered\n",
    "        # this is prefiltering based on 2-grams\n",
    "        data_bigrams = nltk.bigrams(data_words)\n",
    "        data_bigrams = [(w1, w2) for w1, w2 in data_bigrams if w1 not in string.punctuation and w2 not in string.punctuation]\n",
    "        data_bigrams = [(w1, w2) for w1, w2 in data_bigrams if w1 not in \"`’”“\" and w2 not in \"`’”“\"]\n",
    "        data_bigrams = [(w1, w2) for w1, w2 in data_bigrams if w1 not in stop_words and w2 not in stop_words]\n",
    "        data_bigrams = [(w1, w2) for w1, w2 in data_bigrams if nltk.pos_tag(w1)[0][1][:2] in {\"JJ\", \"NN\"} and nltk.pos_tag(w2)[0][1][:2] in {\"JJ\", \"NN\"}]\n",
    "        data_bigrams = [(lem.lemmatize(w1), lem.lemmatize(w2)) for w1, w2 in data_bigrams]\n",
    "        text_words_all_words_noprocess[text_name][2] = data_bigrams\n",
    "        # this is prefiltering based on 3-grams\n",
    "        data_trigrams = nltk.ngrams(data_words, 3)\n",
    "        data_trigrams = [(w1, w2, w3) for w1, w2, w3 in data_trigrams if w1 not in stop_words and w2 not in stop_words and w3 not in stop_words]\n",
    "        data_trigrams = [(w1, w2, w3) for w1, w2, w3 in data_trigrams if w1 not in string.punctuation and  w2 not in string.punctuation and w3 not in string.punctuation]\n",
    "        data_trigrams = [(w1, w2, w3) for w1, w2, w3 in data_trigrams if w1 not in \"`’”“\" and w2 not in \"`’”“\" and w3 not in \"`’”“\"]\n",
    "        data_trigrams = [(w1, w2, w3) for w1, w2, w3 in data_trigrams if nltk.pos_tag([w1])[0][1][:2] in {\"JJ\", \"NN\"} and nltk.pos_tag([w2])[0][1][:2] in {\"JJ\", \"NN\"} and nltk.pos_tag([w3])[0][1][:2] in {\"JJ\", \"NN\"}]\n",
    "        data_trigrams = [(lem.lemmatize(w1), lem.lemmatize(w2), lem.lemmatize(w3)) for w1, w2, w3 in data_trigrams]\n",
    "        text_words_all_words_noprocess[text_name][3] = data_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "848"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many files were preprocessed\n",
    "number_of_files = len(text_words_all_words_noprocess)\n",
    "number_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess all one-grams\n",
    "all_one_grams = []\n",
    "for text_id, info in text_words_all_words_noprocess.items():\n",
    "    all_one_grams += set(info[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_one_grams = collections.Counter(all_one_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_one_grams = set([w for w, count in all_one_grams.items() if count >= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess all one-grams\n",
    "all_two_grams = []\n",
    "for text_id, info in text_words_all_words_noprocess.items():\n",
    "    all_two_grams += set(info[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_two_grams = collections.Counter(all_two_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_two_grams = set([w for w, count in all_two_grams.items() if count >= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess all one-grams\n",
    "all_three_grams = []\n",
    "for text_id, info in text_words_all_words_noprocess.items():\n",
    "    all_three_grams += set(info[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_three_grams = collections.Counter(all_three_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_three_grams = set([w for w, count in all_three_grams.items() if count >= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put all preprocessed words together\n",
    "text_words_all_words_process = collections.defaultdict(dict)\n",
    "for text_id, info in text_words_all_words_noprocess.items():\n",
    "    text_words_all_words_process[text_id][1] = [w for w in info[1] if w in keep_one_grams]\n",
    "    text_words_all_words_process[text_id][2] = [\" \".join(w) for w in info[2] if w in keep_two_grams]\n",
    "    text_words_all_words_process[text_id][3] = [\" \".join(w) for w in info[3] if w in keep_three_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_words_all_words_process_list = collections.defaultdict(list)\n",
    "for text_id, info in text_words_all_words_process.items():\n",
    "    text_words_all_words_process_list[text_id] += info[1]\n",
    "    text_words_all_words_process_list[text_id] += info[2]\n",
    "    text_words_all_words_process_list[text_id] += info[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data fo modeling\n",
    "all_texts = [t for t in text_words_all_words_process_list.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model\n",
    "model_hdp = gensim.models.HdpModel(doc_term_matrix, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check frequency distribution of words by topic\n",
    "all_terms_sum = collections.defaultdict(int)\n",
    "all_topic_terms = model_hdp.show_topics(num_words=300, formatted=False)\n",
    "for topic, all_terms in all_topic_terms:\n",
    "    for term, value in all_terms:\n",
    "        all_terms_sum[term] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate absolute and relative importance of each word to a topic\n",
    "current_model = model_hdp\n",
    "words_represented = dict()\n",
    "for topic_words in current_model.show_topics(num_words=50, formatted=False):\n",
    "    current_topic = topic_words[0]\n",
    "    words_represented[current_topic] = collections.defaultdict(list)\n",
    "    all_words = topic_words[1]\n",
    "    all_words_relative = []\n",
    "    for word, weight in topic_words[1]:\n",
    "        current_sum = all_terms_sum[word]\n",
    "        current_relative_weight = weight/current_sum\n",
    "        all_words_relative.append([current_relative_weight, weight, current_relative_weight*weight, word])\n",
    "    words_represented[current_topic] = all_words_relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'research', 'participant', 'result', 'study']\n",
      "['research', 'bias', 'model', 'cognitive', 'effect']\n",
      "['game', 'player', 'object', 'kind', 'eye']\n",
      "['emotion', 'sound', 'meditation', 'emotional', 'experience']\n",
      "['necessary', 'conscious', 'science', 'scientific', 'dance']\n",
      "['sound', 'duration', 'presentation', 'association', 'fluency']\n",
      "['joy', 'transaction', 'agency', 'legal', 'keyboard']\n",
      "['robot', 'architecture', 'michael', 'statistical method', 'relevance']\n",
      "['ball', 'industry', 'dynamical', 'blood', 'pressure']\n",
      "['gesture', 'visual stimulus', 'immortality', 'well established', 'expectation']\n",
      "['coordinate', 'different scenario', 'theory state', 'frontal cortex', 'prediction error']\n",
      "['tm', 'mep', 'rank', 'le', 'placebo']\n",
      "['receptor', 'subunit', 'learning environment', 'cognitive effort', 'anova']\n",
      "['spectral', 'speed', 'reliability', 'feminine', 'textual']\n",
      "['dorsolateral prefrontal cortex', 'neural correlate', 'inferential', 'theory u', 'thalamus']\n",
      "['sudden', 'spatial smoothing', 'cognitive bias', 'experimenter', 'ultimate goal']\n",
      "['story', 'recent study suggest', 'epistemology', 'expected', 'three group']\n",
      "['selection', 'whether individual', 'pleasure', 'computerized', 'deflection']\n",
      "['two group', 'visual scene', 'sensation', 'secondary', 'school']\n",
      "['mu rhythm', 'rhythm', 'interplay', 'fire', 'mirror']\n"
     ]
    }
   ],
   "source": [
    "# print 5 words with highest socre for each topic\n",
    "for i in range(20):\n",
    "    print(list(pandas.DataFrame(words_represented[i], columns=[\"Relative\", \"Absolute\", \"Score\", \"Word\"]).sort_values(by=\"Score\", ascending=False).head(5)[\"Word\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['research', 'study', 'language', 'participant', 'result']\n",
      "['research', 'cognitive', 'study', 'model', 'participant']\n",
      "['language', 'data', 'study', 'game', 'result']\n",
      "['emotion', 'study', 'social', 'experience', 'subject']\n",
      "['cognitive', 'science', 'research', 'study', 'stimulus']\n",
      "['language', 'sound', 'word', 'cognitive', 'participant']\n",
      "['study', 'brain', 'result', 'method', 'research']\n",
      "['robot', 'learning', 'architecture', 'theory', 'data']\n",
      "['process', 'system', 'research', 'cognitive', 'social']\n",
      "['task', 'motor', 'brain', 'gesture', 'cognitive']\n",
      "['behaviour', 'cognitive', 'study', 'schizophrenia', 'motor']\n",
      "['effect', 'tm', 'motor', 'behaviour', 'task']\n",
      "['cell', 'part', 'status', 'thesis', 'strategy']\n",
      "['picture', 'data', 'spectral', 'network', 'study']\n",
      "['human', 'task', 'system', 'bayesian', 'dorsolateral prefrontal cortex']\n",
      "['music', 'system', 'sudden', 'dominance', 'effect']\n",
      "['story', 'user', 'cognitive', 'theory', 'level']\n",
      "['task', 'word', 'practical', 'comparable', 'stimulus']\n",
      "['school', 'tendency', 'time', 'performance', 'group']\n",
      "['rhythm', 'movement', 'condition', 'mu rhythm', 'motor']\n"
     ]
    }
   ],
   "source": [
    "# print 5 most important words in absolute sense for each topic\n",
    "for i in range(20):\n",
    "    print(list(pandas.DataFrame(words_represented[i], columns=[\"Relative\", \"Absolute\", \"Score\", \"Word\"]).sort_values(by=\"Absolute\", ascending=False).head(5)[\"Word\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
