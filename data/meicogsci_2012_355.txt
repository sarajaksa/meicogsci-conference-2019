... title: Invariant object recognition using perceptron with convolution



Abstract

Often there is need for dividing objects to categories according to their statistical properties. For this reason, technologies for this were developed that allow automatization of this process by machine learning and pattern recognition. One of these are neural network, inspired by principle of functioning of neural system of the humans. They are mainly statistical tool, although used also for other matters. Most universal of them are networks called multilayer perceptrons, that can be used for virtually whichever task that are neural networks suitable in general, because of their ability of learn almost anything, although with the disadvantage of slow learning.
Convolutional neural networks are extented architecture for classical multilayer perceptrons (MLP), with addition of convolution technique that is very common in area of image processing or more generally, in any signal processing. Their architecture is inspired by structure of path which processes information in brain of cat which utilizes recognition of features that are contained in perceived image. They use following procedures:
Receptive fields, that means area of surrounding neurons for one neuron, that influences potential of this single neuron. They can be specialized, for example on orientation or location of patterns in perceived image and have been identified for neurons of the auditory system, the somatosensory system, and the visual system.
Convolution. This is operation that iterates through image pixels and uses so-called kernel, that is actually square matrix of values, on every of them. As a result, depending on kernel values or size, the image gets an "effect". An example of these effects in practice are sharpening or blur that we can common see in any image processing application. In these networks, random kernels are utilized what usually runs into sort of edge detections. This is important to enable detection of features of input image as it can be used as some "guideline" for what it contains.
Sharing of weights. Some neurons between layers of network share their weights that means they have the same weight. This is very useful because it works as feature detector through whole input image. Additionaly, the feature can be rotated and even a bit distorted (noise etc.) and the layer is still capable of its recognition. Also this helps to decrease a number of parameters that have to be trained compared for example to multilayer perceptrons. This can result to smaller computational demands as well.
Subsampling. As the image proceeds through the network, its dimension is being decreased. This is done by pooling techniques, that means by some rule are squares of pixels converted to just one point. In this work, maximum rule is used.
The goal of practical part was to test this kind of network on dataset of leaves from eleven kinds of trees. Five variations of this dataset were used - varying by training samples count and level of distortion for leaves images, in this case the percentage of noise artificially added to images. I wanted to verify whether network is able to cope with these problems in dataset and compare the dataset to other, somehow standardized one to make a notion about its difficulty. The network used consisted of two convolutional layer and one output MLP layer, with convolutional planes number and output layer neurons number varying to find an effect of these on network performance.
As a result, I found the following: I compared the dataset to MNIST dataset of handwritten digits. The leaves dataset seems to be harder overall to recognize, as the testing error for MNIST was under 1%; error for leaves vary from 1% to 8%. This is of course caused by distortions in leaves, although pure leaves with no distortions are harded to recognize as well, as they seem to be more complex. Another thing is that, in general, for big and complex datasets, more convolutional layers and more neurons in output layer helps. The last one is comparation to Deep Belief Network, when convolutional networks seem to be significantly better for these kinds of task, with difference roughly from 5% to 15%.

Bouvrie, J. (2006). Notes on Convolutional Neural Networks. In Practice, 47-60. Citeseer
Johansson, R. (2011). Sorting batteries with deep neural networks. Master thesis, Chalmers University of Technology, Goteborg

