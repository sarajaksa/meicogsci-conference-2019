... title: Model of Sentence Comprehension in Visual Context



Abstract

Traditionally, language comprehension research has focused on purely linguistic processing. Most studies relied on reading methodologies, with reading times interpreted as an indication of the processing complexity of the given sentence. Attention was subsequently given to syntactic parsing mechanisms and how they explain particular sentence processing difficulties. Non-linguistic influences on sentence processing were largely ignored. However, we don't use words and sentences in isolation. Non-linguistic context is omnipresent in human communication, be it our experience and knowledge, or our immediate environment. We need the context to be able to ground the language and put meaning to it.
In the last decade, numerous experiments using the visual world paradigm provided an interesting insight into the interplay of spoken language and visual scene processing (e.g. Mayberry et al., 2009; Crocker et al., 2010). Participants are presented with a scene depicting several events while a spoken utterance regarding one of the events is unfolding. Analysis of eye movements revealed that visual attention is not solely guided by the incremental processing of the unfolding utterance but that participants actively hypothesize about likely role-fillers in the scene, based both on their linguistic and general knowledge and the scene information, and rapidly use the visual information in utterance ambiguity resolution.
The proposed research shall utilize a connectionist model to explore similar phenomena. In this model, visual scene is represented by a self-organizing map with active units representing nouns (girl, bulldozer, house, juice, etc.). Events are composed of a transitive verb (e.g. pushes, points-at) and two nouns (one in the role of agent, other in the role of patient); or a noun and an intransitive verb (e.g. walks, sits). Nouns and verbs are represented by binary feature vectors, with semantic features like size, (in)animateness or mobility for nouns and the presence of motion, physical effort or temporary aspect for verbs. Event representations are created off-line at the hidden layer of an auto-associative neural network. Several events can occur simultaneously in the visual scene. Not all objects presented are necessarily part of some event, some function purely as distractors, while others can participate in multiple events at once.
The scene representation serves as a "visual" input to a simple recurrent network, along with an "auditory" (symbolic) input where the utterance concerning one of the depicted events is presented to the network one word at a time. Arbitrary binary vectors, without any semantic features, are used for "language" representation of individual nouns and verbs.
The network is expected to incrementally construct its own event interpretation on the output layer during the processing of each utterance. In addition, the output from the previous step is  used as a saliency map to modulate the visual input.
During the training phase, a correct interpretation, consisting of the representation of the single event mentioned in the utterance, is presented to the network after finishing a sentence, and the weights are updated using standard back-propagation-through-time algorithm. The network is expected to implicitly learn the relations between the auditory and visual input at this stage.
It is expected that after training the network will exhibit behaviour consistent with the experimental findings mentioned above, namely incrementally constructing a relevant event interpretation during the unfolding of the utterance while inhibiting ambiguous, distracting or irrelevant stimuli. Moreover it is hoped that the model architecture will lead to desirable generalization when the network will be confronted with novel scenes and combinations of objects and events.

Mayberry, M., Crocker, M.W., & Knoeferle, P. (2009). Learning to Attend: A Connectionist Model of the Coordinated Interplay of Utterance, Visual Context, and World  Knowledge. Cognitive Science, 33, 449-496.
Crocker, M.W., Knoeferle, P., & Mayberry, M. (2010). Situated sentence processing: The coordinated interplay account and a neurobehavioral model. Brain and Language 112, 189-201.

